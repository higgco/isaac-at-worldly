{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tier 1 Analysis - Clean Version\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "### 1. Create Virtual Environment\n",
    "```bash\n",
    "# Create virtual environment\n",
    "python -m venv tier-1-venv\n",
    "\n",
    "# Activate virtual environment\n",
    "# On macOS/Linux:\n",
    "source tier-1-venv/bin/activate\n",
    "# On Windows:\n",
    "# tier-1-venv\\Scripts\\activate\n",
    "```\n",
    "\n",
    "### 2. Install Dependencies\n",
    "```bash\n",
    "# Make sure virtual environment is activated\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "### 3. Create .env File\n",
    "Create a `.env` file in the project root with your database credentials:\n",
    "\n",
    "```env\n",
    "DB_USER=your_username\n",
    "DB_PASSWORD=your_password\n",
    "DB_HOST=wg-data-rds.data.higg.org\n",
    "DB_PORT=5432\n",
    "DB_NAME=db_higg\n",
    "```\n",
    "\n",
    "**‚ö†Ô∏è Important:** Replace the placeholder values with your actual database credentials.\n",
    "\n",
    "### 4. Start Jupyter\n",
    "```bash\n",
    "# Make sure virtual environment is activated\n",
    "source tier-1-venv/bin/activate\n",
    "\n",
    "# Start Jupyter\n",
    "jupyter notebook\n",
    "# or\n",
    "jupyter lab\n",
    "```\n",
    "\n",
    "### 5. Select Kernel\n",
    "- Open this notebook\n",
    "- Select **\"Tier 1 Analysis\"** as your kernel\n",
    "- Run the cells in order\n",
    "\n",
    "## Security Notes\n",
    "- **Never commit** the `.env` file to version control\n",
    "- **Keep credentials secure** and don't share them\n",
    "- **Use different credentials** for different environments (dev/staging/prod)\n",
    "\n",
    "## Troubleshooting\n",
    "- **Connection issues**: Verify your database credentials in `.env`\n",
    "- **Kernel not found**: Make sure you've activated the virtual environment\n",
    "- **Package errors**: Run `pip install -r requirements.txt` again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Connected to: wg-data-rds.data.higg.org:5432/db_higg\n",
      "üìÅ SQL file to execute: facility type and pc.sql\n",
      "‚úÖ Environment variables loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Setup: Import libraries, load environment variables, and configure SQL file\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine, text\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configuration: SQL file to execute\n",
    "SQL_FILE = 'facility type and pc.sql'  # Change this to run a different SQL file\n",
    "\n",
    "# Create connection string\n",
    "DB_USER = os.getenv('DB_USER')\n",
    "DB_PASSWORD = os.getenv('DB_PASSWORD')\n",
    "DB_HOST = os.getenv('DB_HOST')\n",
    "DB_PORT = os.getenv('DB_PORT')\n",
    "DB_NAME = os.getenv('DB_NAME')\n",
    "\n",
    "connection_string = f\"postgresql+psycopg2://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
    "\n",
    "print(f\"‚úÖ Connected to: {DB_HOST}:{DB_PORT}/{DB_NAME}\")\n",
    "print(f\"üìÅ SQL file to execute: {SQL_FILE}\")\n",
    "print(\"‚úÖ Environment variables loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Creating database engine...\n",
      "‚úÖ DATABASE CONNECTION SUCCESSFUL!\n",
      "Status: Connection successful!\n",
      "Connected at: 2025-09-09 21:37:57.335180+00:00\n"
     ]
    }
   ],
   "source": [
    "# Create database engine and test connection\n",
    "print(\"üîß Creating database engine...\")\n",
    "\n",
    "try:\n",
    "    # Create engine with optimized settings\n",
    "    engine = create_engine(\n",
    "        connection_string,\n",
    "        pool_pre_ping=True,\n",
    "        pool_recycle=300,\n",
    "        echo=False\n",
    "    )\n",
    "    \n",
    "    # Test connection\n",
    "    with engine.connect() as connection:\n",
    "        result = connection.execute(text(\"SELECT 'Connection successful!' as status, current_timestamp as time\"))\n",
    "        row = result.fetchone()\n",
    "        \n",
    "    print(\"‚úÖ DATABASE CONNECTION SUCCESSFUL!\")\n",
    "    print(f\"Status: {row[0]}\")\n",
    "    print(f\"Connected at: {row[1]}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"‚ùå DATABASE CONNECTION FAILED!\")\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"\\nüîß Check your .env file credentials\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Loading and executing SQL file: facility type and pc.sql\n",
      "‚úÖ SQL file loaded (1461 characters)\n",
      "‚úÖ QUERY EXECUTED SUCCESSFULLY!\n",
      "üìä Results: 5434 rows, 3 columns\n",
      "üìã Columns: ['assessment_id', 'sipfacilityapparelpc', 'apparel_pc_count']\n"
     ]
    }
   ],
   "source": [
    "# Execute your SQL file\n",
    "print(f\"üìÅ Loading and executing SQL file: {SQL_FILE}\")\n",
    "\n",
    "try:\n",
    "    # Read SQL file using the configured variable\n",
    "    with open(SQL_FILE, 'r') as file:\n",
    "        sql_query = file.read()\n",
    "    \n",
    "    print(f\"‚úÖ SQL file loaded ({len(sql_query)} characters)\")\n",
    "    \n",
    "    # Execute query using manual method\n",
    "    with engine.connect() as connection:\n",
    "        result = connection.execute(text(sql_query))\n",
    "        rows = result.fetchall()\n",
    "        columns = result.keys()\n",
    "        \n",
    "    # Create DataFrame\n",
    "    df_results = pd.DataFrame(rows, columns=columns)\n",
    "    \n",
    "    print(\"‚úÖ QUERY EXECUTED SUCCESSFULLY!\")\n",
    "    print(f\"üìä Results: {df_results.shape[0]} rows, {df_results.shape[1]} columns\")\n",
    "    print(f\"üìã Columns: {list(df_results.columns)}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå SQL FILE NOT FOUND: {SQL_FILE}\")\n",
    "    print(\"üîß Make sure the file exists in the current directory\")\n",
    "    print(\"üí° You can change the SQL_FILE variable in Cell 2 to point to a different file\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå QUERY EXECUTION FAILED: {e}\")\n",
    "    print(f\"\\nüîç SQL Query content from {SQL_FILE}:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(sql_query)\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Dataset Overview:\n",
      "   ‚Ä¢ Total rows: 5434\n",
      "   ‚Ä¢ Total columns: 3\n",
      "\n",
      "üìã Column Information:\n",
      "   ‚Ä¢ assessment_id: object (5434 non-null)\n",
      "   ‚Ä¢ sipfacilityapparelpc: object (5434 non-null)\n",
      "   ‚Ä¢ apparel_pc_count: int64 (5434 non-null)\n",
      "\n",
      "üìÑ First 10 rows:\n",
      "                                    assessment_id  \\\n",
      "0  femsurvey:fffff92a-914f-446f-812f-8141dbe416a6   \n",
      "1  femsurvey:ffff536a-d061-4fbd-84af-bfcaf59ac297   \n",
      "2  femsurvey:fff81b83-f145-404b-aeae-d0bb63b0fa1a   \n",
      "3  femsurvey:fff4757f-0b72-4f89-82cb-771a86980e0f   \n",
      "4  femsurvey:ffe917d7-0bf2-4469-af77-cba73a34e513   \n",
      "5  femsurvey:ffe53b8f-d79c-4480-a52f-92f026b318d2   \n",
      "6  femsurvey:ffba25e8-eaea-486e-9abf-6e362be0f88d   \n",
      "7  femsurvey:ffa679a1-848a-465e-92f3-9d518fbe7633   \n",
      "8  femsurvey:ffa0e74d-2f96-4117-b7ba-46680f7741f4   \n",
      "9  femsurvey:ff8205f1-281f-4d6f-8d69-72e447f694ca   \n",
      "\n",
      "                                sipfacilityapparelpc  apparel_pc_count  \n",
      "0          [Hosiery, Pants, Shirts, Skirts, T-shirt]                 5  \n",
      "1                           [Jackets, Pants, Shirts]                 3  \n",
      "2          [Dresses, Pants, Shirts, Skirts, T-shirt]                 5  \n",
      "3  [Baselayers, Hosiery, Pants, Shirts, T-shirt, ...                 6  \n",
      "4                                           [Shirts]                 1  \n",
      "5                [Dresses, Shirts, Skirts, Sweaters]                 4  \n",
      "6              [Dresses, Leggings & Tights, T-shirt]                 3  \n",
      "7  [Dresses, Pants, Shirts, Skirts, T-shirt, Unde...                 6  \n",
      "8                  [Dresses, Jackets, Pants, Shirts]                 4  \n",
      "9                           [Jackets, Pants, Shirts]                 3  \n",
      "\n",
      "üíæ To save results:\n",
      "df_results.to_csv('tier1_results.csv', index=False)\n"
     ]
    }
   ],
   "source": [
    "# Display and analyze results\n",
    "if 'df_results' in locals() and not df_results.empty:\n",
    "    print(\"üìä Dataset Overview:\")\n",
    "    print(f\"   ‚Ä¢ Total rows: {len(df_results)}\")\n",
    "    print(f\"   ‚Ä¢ Total columns: {len(df_results.columns)}\")\n",
    "    \n",
    "    print(\"\\nüìã Column Information:\")\n",
    "    for col in df_results.columns:\n",
    "        dtype = df_results[col].dtype\n",
    "        non_null = df_results[col].count()\n",
    "        print(f\"   ‚Ä¢ {col}: {dtype} ({non_null} non-null)\")\n",
    "    \n",
    "    print(\"\\nüìÑ First 10 rows:\")\n",
    "    print(df_results.head(10))\n",
    "    \n",
    "    print(\"\\nüíæ To save results:\")\n",
    "    print(\"df_results.to_csv('tier1_results.csv', index=False)\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No results available. Run the previous cell first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Loading PIC default product weights...\n",
      "‚úÖ PIC default product weights loaded successfully!\n",
      "üìä Weights data: 14 rows, 3 columns\n",
      "üìã Columns: ['FEM Apparel PC', 'PIC Product', 'Product Weight (kg)']\n",
      "\n",
      "üìÑ PIC Default Product Weights:\n",
      "       FEM Apparel PC  Product Weight (kg)\n",
      "0              Shirts             0.250000\n",
      "1             Dresses             0.374213\n",
      "2             Jackets             0.950000\n",
      "3               Pants             0.453592\n",
      "4              Skirts             0.290299\n",
      "5               Socks             0.400000\n",
      "6            Sweaters             0.550000\n",
      "7           Swimsuits             0.100000\n",
      "8          Baselayers             0.111130\n",
      "9             Hosiery             0.227000\n",
      "10  Leggings & Tights             0.227000\n",
      "11            Jerseys             0.150000\n",
      "12            T-shirt             0.150000\n",
      "13          Underwear             0.138346\n",
      "\n",
      "üîó Created weight mapping for 14 apparel categories:\n",
      "   ‚Ä¢ Shirts: 0.25 kg\n",
      "   ‚Ä¢ Dresses: 0.3742134 kg\n",
      "   ‚Ä¢ Jackets: 0.95 kg\n",
      "   ‚Ä¢ Pants: 0.453592 kg\n",
      "   ‚Ä¢ Skirts: 0.29029888 kg\n",
      "   ‚Ä¢ Socks: 0.4 kg\n",
      "   ‚Ä¢ Sweaters: 0.55 kg\n",
      "   ‚Ä¢ Swimsuits: 0.1 kg\n",
      "   ‚Ä¢ Baselayers: 0.11113004 kg\n",
      "   ‚Ä¢ Hosiery: 0.22700006 kg\n",
      "   ‚Ä¢ Leggings & Tights: 0.22700006 kg\n",
      "   ‚Ä¢ Jerseys: 0.15 kg\n",
      "   ‚Ä¢ T-shirt: 0.15 kg\n",
      "   ‚Ä¢ Underwear: 0.13834556 kg\n",
      "\n",
      "üí° Usage example:\n",
      "   default_weight = apparel_weights.get('Shirts', 0.25)  # Returns 0.25 if 'Shirts' not found\n"
     ]
    }
   ],
   "source": [
    "# Load PIC default product weights for apparel categories\n",
    "print(\"üìÅ Loading PIC default product weights...\")\n",
    "\n",
    "try:\n",
    "    # Load the CSV file (it's actually tab-separated)\n",
    "    weights_df = pd.read_csv('PIC default product weights.csv', sep='\\t')\n",
    "    \n",
    "    print(\"‚úÖ PIC default product weights loaded successfully!\")\n",
    "    print(f\"üìä Weights data: {weights_df.shape[0]} rows, {weights_df.shape[1]} columns\")\n",
    "    print(f\"üìã Columns: {list(weights_df.columns)}\")\n",
    "    \n",
    "    # Display the weights data (only FEM Apparel PC and Product Weight columns)\n",
    "    print(\"\\nüìÑ PIC Default Product Weights:\")\n",
    "    display_df = weights_df[['FEM Apparel PC', 'Product Weight (kg)']]\n",
    "    print(display_df)\n",
    "    \n",
    "    # Create a dictionary mapping for easy lookup\n",
    "    apparel_weights = dict(zip(weights_df['FEM Apparel PC'], weights_df['Product Weight (kg)']))\n",
    "    \n",
    "    print(f\"\\nüîó Created weight mapping for {len(apparel_weights)} apparel categories:\")\n",
    "    for category, weight in apparel_weights.items():\n",
    "        print(f\"   ‚Ä¢ {category}: {weight} kg\")\n",
    "     \n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå PIC default product weights.csv file not found!\")\n",
    "    print(\"üîß Make sure the file exists in the current directory\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading weights file: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tier 1 Analysis",
   "language": "python",
   "name": "tier-1-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
